#!/bin/bash
set -euo pipefail # Strict mode

# --- Logging Setup ---
LOG_FILE="/var/log/worker-node-bootstrap.log" # Specific name for this script's log
touch "${LOG_FILE}"
chmod 644 "${LOG_FILE}"
exec > >(tee -a "${LOG_FILE}") 2>&1 # Redirect stdout and stderr to log file and console

# --- Color Codes and Logging Functions ---
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
NC='\033[0m' # No Color

log() { echo -e "${GREEN}[INFO]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"; }
warn() { echo -e "${YELLOW}[WARNING]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"; }
error() { echo -e "${RED}[ERROR]${NC} $(date '+%Y-%m-%d %H:%M:%S') - $1"; exit 1; }

log "Worker Node User Data Script (for unbaked AMI) started."

# --- Configuration Variables (Passed as command-line arguments) ---

# Check if the required number of arguments are provided
if [ "$#" -lt 3 ]; then
    echo "Usage: $0 <target_user> <k8s_repo_stream> <k8s_package_version_string> [optional_arg4]"
    echo "Error: Missing required arguments."
    exit 1
fi

TARGET_USER="$1"
K8S_REPO_STREAM="$2"                 # e.g., 1.33 (used for repo URL)
K8S_PKG_VERSION_STRING="$3"          # e.g., 1.33.1-00 (used for apt install)
# OPTIONAL_ARG4="$4"                 # Uncomment and use if ARG4 is needed

# --- Example: Output the received arguments (for verification) ---
echo "Received TARGET_USER: ${TARGET_USER}"
echo "Received K8S_REPO_STREAM: ${K8S_REPO_STREAM}"
echo "Received K8S_PKG_VERSION_STRING: ${K8S_PKG_VERSION_STRING}"
if [ -n "$4" ]; then
    echo "Received OPTIONAL_ARG4: $4"
fi

# --- Fetch Instance Private IP ---
echo "Fetching instance private IP from metadata..."
INSTANCE_PRIVATE_IP=$(curl -s http://169.254.169.254/latest/meta-data/local-ipv4)
if [ -z "$INSTANCE_PRIVATE_IP" ]; then
    echo "FATAL: Could not determine instance private IP from metadata service."
    exit 1
fi
echo "Instance private IP: ${INSTANCE_PRIVATE_IP}"
# --- End Fetch Instance Private IP ---


echo "Setting up user: ${TARGET_USER}

# --- User Setup ---
# Create user if they don't exist
if ! id "${TARGET_USER}" &>/dev/null; then
    echo "Creating user ${TARGET_USER}..."
    useradd -m -s /bin/bash "${TARGET_USER}"
    passwd -d "${TARGET_USER}" # Ensure password login is disabled if desired
else
    echo "User ${TARGET_USER} already exists."
fi

# Configure passwordless sudo
echo "${TARGET_USER} ALL=(ALL) NOPASSWD:ALL" > "/etc/sudoers.d/${TARGET_USER}" # Use > instead of | sudo tee
chmod 440 "/etc/sudoers.d/${TARGET_USER}"

# Set up SSH key authentication from EC2 metadata
SSH_PUBLIC_KEY=$(curl -fsSL http://169.254.169.254/latest/meta-data/public-keys/0/openssh-key || echo "Failed to fetch SSH key from metadata")
if [[ -n "$SSH_PUBLIC_KEY" && "$SSH_PUBLIC_KEY" != "Failed to fetch"* ]]; then
    USER_SSH_DIR="/home/${TARGET_USER}/.ssh"
    mkdir -p "${USER_SSH_DIR}"
    echo "${SSH_PUBLIC_KEY}" > "${USER_SSH_DIR}/authorized_keys"
    chmod 700 "${USER_SSH_DIR}"
    chmod 600 "${USER_SSH_DIR}/authorized_keys"
    chown -R "${TARGET_USER}:${TARGET_USER}" "${USER_SSH_DIR}" # Assuming group name matches username
    echo "SSH key configured for ${TARGET_USER}"
else
    echo "Warning: Could not retrieve SSH public key for ${TARGET_USER}."
    # Decide if this is a fatal error? If SSH is essential, add 'exit 1'
fi

echo "Starting Kubernetes setup on EC2..."

# Disable swap
swapoff -a
sed -i.bak '/ swap / s/^/#/' /etc/fstab # Added .bak for safety

# Enable kernel modules
modprobe overlay
modprobe br_netfilter
echo -e "overlay\nbr_netfilter" > /etc/modules-load.d/k8s.conf # Use > instead of tee
echo "net.ipv4.ip_forward = 1" | sudo tee /etc/sysctl.d/k8s.conf

# Configure sysctl params
echo "net.bridge.bridge-nf-call-iptables = 1" > /etc/sysctl.d/k8s.conf # Added bridge setting often needed
echo "net.ipv4.ip_forward = 1" >> /etc/sysctl.d/k8s.conf # Use >> to append
sysctl --system # Reload sysctl settings


# Install dependencies
export DEBIAN_FRONTEND=noninteractive
apt-get update
apt-get install -y apt-transport-https ca-certificates curl gpg awscli jq

# Install Kubernetes (using specific version passed from Terraform)
K8S_KEYRING_DIR="/etc/apt/keyrings"
K8S_KEYRING_FILE="${K8S_KEYRING_DIR}/kubernetes-apt-keyring.gpg"
K8S_REPO_FILE="/etc/apt/sources.list.d/kubernetes.list"

echo "Setting up Kubernetes APT repository for stream v${K8S_REPO_STREAM} ...."
sudo mkdir -p "${K8S_KEYRING_DIR}"
sudo curl -fsSL "https://pkgs.k8s.io/core:/stable:/v${K8S_REPO_STREAM}/deb/Release.key" | sudo gpg --dearmor -o "${K8S_KEYRING_FILE}" || { echo "Failed to download k8s gpg key for stream v${K8S_REPO_STREAM}"; exit 1; }
echo "deb [signed-by=${K8S_KEYRING_FILE}] https://pkgs.k8s.io/core:/stable:/v${K8S_REPO_STREAM}/deb/ /" | sudo tee "${K8S_REPO_FILE}" > /dev/null || { echo "Failed to write k8s repo file"; exit 1; }

sudo apt-get update

echo "Installing Kubernetes packages: kubelet=${K8S_PKG_VERSION_STRING}, kubeadm=${K8S_PKG_VERSION_STRING}, kubectl=${K8S_PKG_VERSION_STRING}"
sudo apt-get install -y \
  kubelet="${K8S_PKG_VERSION_STRING}" \
  kubeadm="${K8S_PKG_VERSION_STRING}" \
  kubectl="${K8S_PKG_VERSION_STRING}" || {
    echo "Failed to install specific Kubernetes package versions. Trying to find them..."
    echo "Available kubeadm versions:"
    apt-cache madison kubeadm
    exit 1
  }

apt-mark hold kubelet kubeadm kubectl

# Install and configure containerd
CONTAINERD_KEYRING_DIR="/usr/share/keyrings"
CONTAINERD_KEYRING_FILE="${CONTAINERD_KEYRING_DIR}/docker-archive-keyring.gpg"
CONTAINERD_REPO_FILE="/etc/apt/sources.list.d/docker.list"

curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o "${CONTAINERD_KEYRING_FILE}" || { echo "Failed to download docker/containerd gpg key"; exit 1; }
echo "deb [arch=$(dpkg --print-architecture) signed-by=${CONTAINERD_KEYRING_FILE}] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" > "${CONTAINERD_REPO_FILE}" || { echo "Failed to write containerd repo file"; exit 1; }

apt-get update
apt-get install -y containerd.io


echo "Configuring containerd for Kubernetes..."

# Ensure the config directory exists
sudo mkdir -p /etc/containerd

# Generate default config and save it
echo "Generating default containerd config -> /etc/containerd/config.toml"
sudo bash -c 'containerd config default > /etc/containerd/config.toml'

# 1. Comment out the 'disabled_plugins = ["cri"]' line using sed
echo "Ensuring CRI plugin is NOT disabled in containerd config..."
# This sed command finds the exact line and puts a '#' at the beginning
sudo sed -i 's/^disabled_plugins = \["cri"\]/#disabled_plugins = ["cri"]/' /etc/containerd/config.toml

# 2. Modify the config to enable SystemdCgroup (keep this from before)
echo "Enabling SystemdCgroup in containerd config..."
sudo sed -i 's/SystemdCgroup = false/SystemdCgroup = true/' /etc/containerd/config.toml

# Restart containerd to apply changes
echo "Restarting containerd service..."
sudo systemctl restart containerd

# Ensure containerd service is enabled on boot
echo "Enabling containerd service..."
sudo systemctl enable containerd

# Optional: Brief wait/check for socket
echo "Waiting briefly for containerd socket..."
sleep 5
if [ ! -S /var/run/containerd/containerd.sock ]; then
  echo "WARNING: containerd socket not found after restart!"
fi

echo "Containerd configured and restarted."
# --- End containerd configuration section ---

echo "-----------------------------------------------------"
echo "Kubernetes control plane setup completed successfully!"
echo "-----------------------------------------------------"


# --- Configuration Variables (Path Definitions) ---
NVME_MOUNT_POINT="/mnt/nvme_storage"
DEFAULT_AMI_USER="ubuntu" # Or ec2-user, adjust if your base AMI's default user differs

# K8s core directories on NVMe
DEFAULT_CONTAINERD_DIR="/var/lib/containerd"
NVME_CONTAINERD_DIR="${NVME_MOUNT_POINT}/lib/containerd"

DEFAULT_KUBELET_DIR="/var/lib/kubelet"
NVME_KUBELET_DIR="${NVME_MOUNT_POINT}/lib/kubelet"

DEFAULT_K8S_POD_LOGS_DIR="/var/log/pods"
NVME_K8S_POD_LOGS_DIR="${NVME_MOUNT_POINT}/log/pods"

# For TARGET_USER (e.g., graphscope)
SHARED_MODELS_BASE_ON_NVME="${NVME_MOUNT_POINT}/shared_data/${TARGET_USER}"
HUGGINGFACE_HOME_ON_NVME="${SHARED_MODELS_BASE_ON_NVME}/huggingface"
DATASETS_PATH_ON_NVME="${SHARED_MODELS_BASE_ON_NVME}/datasets"
LLM_MODELS_PATH_ON_NVME="${SHARED_MODELS_BASE_ON_NVME}/llm_models"
LOCAL_PATH_PROVISIONER_DIR_ON_NVME="${NVME_MOUNT_POINT}/local-path-provisioner"

# User Docker Data (if Docker CE was installed in AMI for the TARGET_USER)
USER_DOCKER_DATA_ROOT_ON_NVME="${NVME_MOUNT_POINT}/user_docker_data/${TARGET_USER}"

# --- 1. Detect, Format, and Mount Ephemeral NVMe ---
log "Starting NVMe drive setup..."
ROOT_PARTITION=$(findmnt -n -o SOURCE /) || error "Cannot find root partition."
log "Root partition detected: ${ROOT_PARTITION}"
ROOT_DISK_NAME=$(lsblk -no pkname "${ROOT_PARTITION}")
ROOT_DISK_PATH_PREFIX="/dev/"
ROOT_DISK=""

if [ -n "${ROOT_DISK_NAME}" ]; then
    if [[ ! "${ROOT_DISK_NAME}" == /* ]]; then # Check if it's a relative name like "nvme0n1"
      ROOT_DISK="${ROOT_DISK_PATH_PREFIX}${ROOT_DISK_NAME}"
    else # It's already a full path like "/dev/nvme0n1"
      ROOT_DISK="${ROOT_DISK_NAME}"
    fi
else
    # Fallback if pkname failed (e.g., root is directly on a device without partition table like /dev/xvda)
    # Attempt to derive from ROOT_PARTITION by removing partition numbers if any
    # This is a heuristic and might need adjustment based on specific device naming schemes
    # For /dev/nvme0n1p1 -> /dev/nvme0n1
    # For /dev/xvda1 -> /dev/xvda
    DERIVED_DISK_NAME=$(echo "${ROOT_PARTITION}" | sed -E 's/p[0-9]+$//' | sed -E 's/[0-9]+$//')
    if [[ -n "$DERIVED_DISK_NAME" && -b "$DERIVED_DISK_NAME" ]]; then # Check if it's a block device
        ROOT_DISK="$DERIVED_DISK_NAME"
    elif [[ -b "${ROOT_PARTITION}" ]]; then # If ROOT_PARTITION itself is a block device (e.g. no partitions)
        ROOT_DISK="${ROOT_PARTITION}"
    else
        error "Could not confidently determine root disk from root partition ${ROOT_PARTITION}."
    fi
fi
log "Root disk identified as: ${ROOT_DISK}"


TARGET_NVME_DEVICE=""
MIN_SIZE_BYTES=107374182400 # 100 GiB
# lsblk options: -d (no partitions), -p (full paths), -b (bytes), -n (no headers), -o (columns), -e 7 (exclude loop)
while read -r DEV TYPE SIZE MOUNTPOINT_LSBLK; do
    log "Checking device for NVMe: Name=${DEV}, Type=${TYPE}, Size=${SIZE}, Mountpoint='${MOUNTPOINT_LSBLK}'"
    # Ensure DEV is a block device, starts with /dev/nvme, is a disk, not the ROOT_DISK, and meets size criteria
    if [[ -b "${DEV}" && "${DEV}" == /dev/nvme* && "${TYPE}" == "disk" && "${DEV}" != "${ROOT_DISK}" && "${SIZE}" -gt "${MIN_SIZE_BYTES}" ]]; then
        if [ -z "${MOUNTPOINT_LSBLK}" ] || [ "${MOUNTPOINT_LSBLK}" == "/mnt" ] || [[ "${MOUNTPOINT_LSBLK}" == /media/ephemeral* ]]; then
            log "Found candidate ephemeral NVMe device: ${DEV}"
            TARGET_NVME_DEVICE="${DEV}"
            if [ -n "${MOUNTPOINT_LSBLK}" ] && [ "${MOUNTPOINT_LSBLK}" != "/" ]; then # Don't unmount root!
                log "Unmounting ${DEV} from temporary mount ${MOUNTPOINT_LSBLK}..."
                umount "${DEV}" || warn "Could not unmount ${DEV} from ${MOUNTPOINT_LSBLK}. Proceeding with format."
            fi
            break
        else
            log "Device ${DEV} has unexpected mountpoint '${MOUNTPOINT_LSBLK}'. Skipping."
        fi
    fi
done < <(lsblk -dpbno NAME,TYPE,SIZE,MOUNTPOINT -e 7)

if [ -z "${TARGET_NVME_DEVICE}" ]; then
    lsblk -fp # Log full output for debugging if NVMe not found
    error "CRITICAL: Could not dynamically determine a suitable target NVMe device. Ensure instance type has appropriate NVMe instance storage."
fi
NVME_DEVICE="${TARGET_NVME_DEVICE}"
log "Target NVMe device for formatting and use: ${NVME_DEVICE}"

log "Formatting ${NVME_DEVICE} with xfs..."
mkfs.xfs -f "${NVME_DEVICE}" || error "Failed to format ${NVME_DEVICE} with xfs."

mkdir -p "${NVME_MOUNT_POINT}"

log "Mounting ${NVME_DEVICE} to ${NVME_MOUNT_POINT}..."
mount -t xfs -o discard "${NVME_DEVICE}" "${NVME_MOUNT_POINT}" || error "Failed to mount ${NVME_DEVICE} to ${NVME_MOUNT_POINT}."

log "NVMe storage at ${NVME_MOUNT_POINT} is now formatted and mounted."
df -hT "${NVME_MOUNT_POINT}"

# --- 2. Create Directory Structures on Mounted NVMe ---
log "Creating K8s and user data directories on ${NVME_MOUNT_POINT}..."
mkdir -p \
    "${NVME_CONTAINERD_DIR}" \
    "${NVME_KUBELET_DIR}" \
    "${NVME_K8S_POD_LOGS_DIR}" \
    "${HUGGINGFACE_HOME_ON_NVME}" \
    "${DATASETS_PATH_ON_NVME}" \
    "${LLM_MODELS_PATH_ON_NVME}" \
    "${LOCAL_PATH_PROVISIONER_DIR_ON_NVME}" || error "Failed to create one or more base directories on NVMe."

chmod 1777 "${LOCAL_PATH_PROVISIONER_DIR_ON_NVME}"
log "Set permissions for ${LOCAL_PATH_PROVISIONER_DIR_ON_NVME}."

if id "${TARGET_USER}" &>/dev/null; then
    mkdir -p "$(dirname "${SHARED_MODELS_BASE_ON_NVME}")"
    # Chown the parent of SHARED_MODELS_BASE_ON_NVME first, then the directory itself
    # Example: /mnt/nvme_storage/shared_data/
    chown "${TARGET_USER}:${TARGET_USER}" "$(dirname "${SHARED_MODELS_BASE_ON_NVME}")" || warn "Failed to chown $(dirname "${SHARED_MODELS_BASE_ON_NVME}")"
    # Example: /mnt/nvme_storage/shared_data/graphscope/
    chown -R "${TARGET_USER}:${TARGET_USER}" "${SHARED_MODELS_BASE_ON_NVME}" || warn "Failed to chown ${SHARED_MODELS_BASE_ON_NVME}"
    chmod -R u=rwX,g=rX,o= "${SHARED_MODELS_BASE_ON_NVME}"
    log "Set ownership and permissions for ${TARGET_USER}'s shared data directories."
else
    warn "User ${TARGET_USER} not found! Skipping chown for shared data directories. This user should exist from AMI."
fi

if [ -f "/etc/docker/daemon.json" ]; then
    log "Creating user Docker data directory on NVMe: ${USER_DOCKER_DATA_ROOT_ON_NVME}"
    mkdir -p "${USER_DOCKER_DATA_ROOT_ON_NVME}"
    if id "${TARGET_USER}" &>/dev/null; then
        chown -R "${TARGET_USER}:${TARGET_USER}" "${USER_DOCKER_DATA_ROOT_ON_NVME}"
    else
        warn "User ${TARGET_USER} not found! User Docker data directory will be root-owned."
    fi
fi

# --- 3. Stop Services (Temporarily for Symlinking) ---
log "Stopping services temporarily for symlink setup..."
if systemctl list-units --full -all | grep -q 'docker.service'; then
    if systemctl is-active --quiet docker; then
        systemctl stop docker.socket docker.service || warn "Failed to stop Docker service."
    else
        log "Docker service found but not active."
    fi
else
    log "Docker service not found, skipping stop for it."
fi
# Kubelet and Containerd might not be running on a fresh AMI before join, errors are warnings.
systemctl stop kubelet || warn "kubelet not running or failed to stop (expected before join)."
systemctl stop containerd || warn "containerd not running or failed to stop."
sleep 3

# --- 4. Create Symlinks ---
create_symlink_if_not_exists() {
    local target_path="$1"
    local link_path="$2"
    local service_name="$3"

    log "Processing symlink for ${service_name}: ${link_path} -> ${target_path}"
    mkdir -p "$(dirname "${link_path}")"

    if [ -L "${link_path}" ]; then
        if [ "$(readlink -f "${link_path}")" = "${target_path}" ]; then
            log "Symlink ${link_path} already exists and is correct for ${service_name}."
            return 0
        else
            warn "Symlink ${link_path} exists but points to $(readlink -f "${link_path}"). Removing and re-creating for ${service_name}."
            rm -f "${link_path}" || warn "Failed to remove existing incorrect symlink ${link_path}"
        fi
    elif [ -e "${link_path}" ]; then
        warn "${link_path} exists but is not a symlink (it's a file or dir). Backing up and re-creating for ${service_name}."
        # Check if directory is not empty before attempting to move.
        if [ -d "${link_path}" ] && [ -n "$(ls -A "${link_path}")" ]; then
             mv "${link_path}" "${link_path}.bak_$(date +%s)_userdata" || warn "Failed to backup non-empty ${link_path}"
        elif [ -d "${link_path}" ]; then # Empty directory
             rm -rf "${link_path}" || warn "Failed to remove empty directory ${link_path}"
        else # It's a file
             mv "${link_path}" "${link_path}.bak_$(date +%s)_userdata" || warn "Failed to backup file ${link_path}"
        fi
    fi

    log "Creating symlink ${link_path} -> ${target_path} for ${service_name}."
    ln -snf "${target_path}" "${link_path}" || error "Failed to create symlink ${link_path} for ${service_name}."
    chown -h root:root "${link_path}"
}

log "Creating symlinks for Kubernetes core directories..."
create_symlink_if_not_exists "${NVME_CONTAINERD_DIR}" "${DEFAULT_CONTAINERD_DIR}" "containerd"
create_symlink_if_not_exists "${NVME_KUBELET_DIR}" "${DEFAULT_KUBELET_DIR}" "kubelet"
create_symlink_if_not_exists "${NVME_K8S_POD_LOGS_DIR}" "${DEFAULT_K8S_POD_LOGS_DIR}" "pod-logs"

# --- 5. Restart Services ---
log "Restarting containerd..."
systemctl start containerd || error "Failed to restart containerd after symlink setup."

if systemctl list-units --full -all | grep -q 'docker.service'; then
    if [ -f "/etc/docker/daemon.json" ]; then
        log "Restarting user Docker service..."
        systemctl start docker.socket docker.service || warn "Failed to restart Docker service."
    fi
fi

# --- 6. (Optional) Setup SSH for TARGET_USER ---
log "Setting up SSH authorized_keys for ${TARGET_USER} by copying from ${DEFAULT_AMI_USER}..."
USER_HOME_FOR_SSH="/home/${TARGET_USER}"
USER_SSH_DIR_FOR_SSH="${USER_HOME_FOR_SSH}/.ssh"
DEFAULT_USER_AUTHORIZED_KEYS_PATH="/home/${DEFAULT_AMI_USER}/.ssh/authorized_keys"

if [ -f "${DEFAULT_USER_AUTHORIZED_KEYS_PATH}" ]; then
    if id "${TARGET_USER}" &>/dev/null; then
        mkdir -p "${USER_SSH_DIR_FOR_SSH}"
        cp "${DEFAULT_USER_AUTHORIZED_KEYS_PATH}" "${USER_SSH_DIR_FOR_SSH}/authorized_keys"
        chown -R "${TARGET_USER}:${TARGET_USER}" "${USER_SSH_DIR_FOR_SSH}"
        chmod 700 "${USER_SSH_DIR_FOR_SSH}"
        chmod 600 "${USER_SSH_DIR_FOR_SSH}/authorized_keys"
        log "SSH authorized_keys copied for ${TARGET_USER} from ${DEFAULT_AMI_USER}."
    else
        warn "User ${TARGET_USER} does not exist. Cannot set up SSH keys for them."
    fi
else
    warn "Default user (${DEFAULT_AMI_USER}) authorized_keys not found at ${DEFAULT_USER_AUTHORIZED_KEYS_PATH}. Cannot copy for ${TARGET_USER}."
fi

# --- 7. Fetch and Execute kubeadm join ---
log "Fetching kubeadm join command from SSM: ${SSM_JOIN_COMMAND_PATH}"

IMDSV2_TOKEN=""
# Attempt to get IMDSv2 token, but proceed if it fails (e.g. IMDSv1 allowed or IMDS disabled)
IMDSV2_TOKEN=$(curl -s -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 300" || true)
METADATA_HEADER_ARGS=""
if [ -n "$IMDSV2_TOKEN" ]; then
    METADATA_HEADER_ARGS="-H \"X-aws-ec2-metadata-token: $IMDSV2_TOKEN\""
else
    warn "Failed to get IMDSv2 token. Proceeding without it. Ensure EC2 instance metadata service is accessible."
fi

EC2_REGION=$(eval "curl -s $METADATA_HEADER_ARGS http://169.254.169.254/latest/dynamic/instance-identity/document" | jq -r .region)
if [ -z "$EC2_REGION" ] || [ "$EC2_REGION" == "null" ]; then
    warn "Failed to determine EC2 region from metadata. Defaulting to us-east-1. Set explicitly if needed."
    EC2_REGION="us-east-1" # Fallback region
fi


if ! command -v aws &> /dev/null; then
    error "AWS CLI not found. This should have been in the baked AMI."
fi
if ! command -v jq &> /dev/null; then
    error "jq not found. This should have been in the baked AMI."
fi

log "Fetching join command from SSM Path: ${SSM_JOIN_COMMAND_PATH} in region ${EC2_REGION}"
JOIN_COMMAND=$(aws ssm get-parameter --name "${SSM_JOIN_COMMAND_PATH}" --with-decryption --query Parameter.Value --output text --region "${EC2_REGION}")

if [ -z "$JOIN_COMMAND" ]; then
    error "Failed to retrieve join command from SSM: ${SSM_JOIN_COMMAND_PATH}"
fi

log "Executing kubeadm join command..."
eval "${JOIN_COMMAND}" # Consider adding --v=5 for debugging if needed: eval "${JOIN_COMMAND} --v=5"
JOIN_EXIT_CODE=$?

if [ $JOIN_EXIT_CODE -ne 0 ]; then
    error "kubeadm join command failed with exit code $JOIN_EXIT_CODE."
fi

log "kubeadm join command completed successfully."
sleep 10
if systemctl is-active --quiet kubelet; then
    log "Kubelet service is active after join."
else
    warn "Kubelet service is NOT active after join. Check 'journalctl -u kubelet' for errors."
fi

log "Worker node User Data script finished successfully."
exit 0
