
#!/bin/bash
#
# This script configures a Kubernetes controller node.
# It's designed to be used as a Terraform templatefile.
#
# It performs one of two actions based on the 'is_primary_controller' variable:
#   - TRUE: Initializes the first controller node using kubeadm init.
#   - FALSE: Joins a secondary controller node to an existing cluster.
#

# Exit immediately if a command exits with a non-zero status.
set -e
# Treat unset variables as an error when substituting.
set -u
# Pipestatus is non-zero if any command in a pipeline fails.
set -o pipefail

echo "DEBUG: The SSM join command path passed to this script is: '${ssm_join_command_path}'"

# --- Setup Logging ---
# Create a dedicated log directory and file for this provisioning script.
sudo mkdir -p /var/log/terraform-provisioning
sudo chown "$(whoami):$(whoami)" /var/log/terraform-provisioning
LOG_FILE="/var/log/terraform-provisioning/configure-controller-${node_index}.log"
touch "$LOG_FILE"
# Redirect all stdout and stderr to the log file and also to the console.
exec &> >(tee -a "$LOG_FILE")

echo "================================================="
echo "=== K8s Controller Configuration Starting ==="
echo "    Run started at: $(date)"
echo "    Node Index: ${node_index}"
echo "    Is Primary Controller: ${is_primary_controller}"
echo "================================================="

# --- Helper Functions ---
handle_error() {
  local error_message="$1"
  echo "ERROR: $${error_message}"
  echo "Configuration failed at $(date)"
  exit 1
}

# --- Dependency Installation ---
echo "Checking for required dependencies..."
if ! command -v jq &> /dev/null; then
  echo "jq not found. Installing..."
  sudo apt-get update && sudo apt-get install -y jq || handle_error "Failed to install jq"
fi

if ! command -v aws &> /dev/null; then
  echo "AWS CLI not found. Installing..."
  curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip" || handle_error "Failed to download AWS CLI"
  unzip awscliv2.zip || handle_error "Failed to unzip AWS CLI"
  sudo ./aws/install || handle_error "Failed to install AWS CLI"
fi
echo "Dependencies are present."

# --- Fetch Instance Metadata ---
echo "Fetching instance metadata using IMDSv2..."
# Request a token for the metadata service
TOKEN=$(curl -X PUT "http://169.254.169.254/latest/api/token" -H "X-aws-ec2-metadata-token-ttl-seconds: 21600" -s --max-time 10) || handle_error "Failed to get IMDSv2 token"
# Fetch instance IP and Region using the token
INSTANCE_IP=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s --max-time 10 http://169.254.169.254/latest/meta-data/local-ipv4) || handle_error "Failed to get instance IP"
INSTANCE_REGION=$(curl -H "X-aws-ec2-metadata-token: $TOKEN" -s --max-time 10 http://169.254.169.254/latest/dynamic/instance-identity/document | jq -r .region) || handle_error "Failed to get instance region"
echo "Instance IP: $INSTANCE_IP"
echo "Instance Region: $INSTANCE_REGION"

# Verify that the IAM role credentials are working
echo "Verifying AWS credentials..."
aws sts get-caller-identity --region "$INSTANCE_REGION" > /dev/null || handle_error "AWS credentials are not working. Check the IAM role."


# ==============================================================================
# --- MAIN LOGIC: Differentiate between Primary and Secondary Controllers ---
# ==============================================================================
# The 'is_primary_controller' variable is passed in from Terraform.
# This avoids hardcoding logic based on IP addresses.

%{ if is_primary_controller ~}
# --- PRIMARY CONTROLLER (INDEX 0) LOGIC ---
echo "This is the PRIMARY controller. Starting initialization."

# Generate the kubeadm configuration file for initialization.
echo "Generating kubeadm-config-rendered.yaml..."
cat > /tmp/kubeadm-config-rendered.yaml << EOF
apiVersion: kubeadm.k8s.io/v1beta3
kind: InitConfiguration
nodeRegistration:
  kubeletExtraArgs: {}
---
apiVersion: kubeadm.k8s.io/v1beta3
kind: ClusterConfiguration
kubernetesVersion: v${k8s_full_patch_version}
controlPlaneEndpoint: "${primary_controller_ip}:6443"
apiServer:
  certSANs:
  - "${primary_controller_ip}"
  - "kubernetes"
  - "kubernetes.default"
  - "kubernetes.default.svc"
  - "kubernetes.default.svc.cluster.local"
  - "localhost"
  - "127.0.0.1"
networking:
  podSubnet: ${pod_cidr_block}
  serviceSubnet: ${service_cidr_block}
EOF

# Verify the bootstrap script (containing kubeadm init) exists
if [ ! -f /tmp/bootstrap.sh ]; then
  handle_error "Bootstrap script /tmp/bootstrap.sh not found. It should have been uploaded by a previous provisioner."
fi

echo "Running bootstrap script for primary controller..."
chmod +x /tmp/bootstrap.sh
# The bootstrap.sh script is expected to run 'kubeadm init'
sudo bash /tmp/bootstrap.sh || handle_error "Primary controller bootstrap script failed"

echo "Primary controller bootstrap completed!"

%{ else ~}
# --- SECONDARY CONTROLLER LOGIC ---
echo "This is a SECONDARY controller (index ${node_index}). Starting join process."

# Wait for the primary controller's API server to become available.
echo "Waiting for primary controller API server at https://${primary_controller_ip}:6443..."
TIMEOUT=600 # 10 minutes
ELAPSED=0
INTERVAL=15
while ! curl -k --max-time 5 "https://${primary_controller_ip}:6443/healthz" &>/dev/null; do
  if [ $ELAPSED -ge $TIMEOUT ]; then
    handle_error "Timeout waiting for primary controller at ${primary_controller_ip}:6443"
  fi
  echo "Waiting for primary API... ($ELAPSED/$TIMEOUT seconds)"
  sleep $INTERVAL
  ELAPSED=$((ELAPSED + INTERVAL))
done
echo "Primary controller API is ready!"

# Check if this node is already part of a cluster to ensure idempotency.
if sudo test -f /etc/kubernetes/kubelet.conf; then
  echo "Node already appears to be part of a cluster. Exiting successfully."
  exit 0
fi

# Fetch the join command and certificate key from AWS SSM Parameter Store.
# These should have been uploaded by the primary controller after it initialized.
echo "Fetching join information from SSM Parameter Store..."
WORKER_JOIN_COMMAND=$(aws ssm get-parameter --name "${ssm_join_command_path}" --with-decryption --query Parameter.Value --output text --region "$INSTANCE_REGION" 2>/dev/null) || handle_error "Failed to run SSM command for join command. Check IAM permissions."
CERTIFICATE_KEY=$(aws ssm get-parameter --name "${ssm_certificate_key_path}" --with-decryption --query Parameter.Value --output text --region "$INSTANCE_REGION" 2>/dev/null) || handle_error "Failed to run SSM command for certificate key. Check IAM permissions."

# Validate that the fetched values are not empty or null.
if [ -z "$WORKER_JOIN_COMMAND" ] || [ "$WORKER_JOIN_COMMAND" = "None" ]; then
  handle_error "Worker join command from SSM is empty or None. Did the primary controller fail to upload it?"
fi
if [ -z "$CERTIFICATE_KEY" ] || [ "$CERTIFICATE_KEY" = "None" ]; then
  handle_error "Certificate key from SSM is empty or None. Did the primary controller fail to upload it?"
fi
echo "Successfully retrieved join information from SSM."

# Extract just the arguments from the full join command.
JOIN_ARGS=$(echo "$WORKER_JOIN_COMMAND" | sed 's/kubeadm join //')
if [ -z "$JOIN_ARGS" ]; then
  handle_error "Failed to extract join arguments from the full command."
fi

# Wait for essential services on this node to be ready before attempting to join.
echo "Waiting for containerd service to be active..."
timeout 300 bash -c 'until sudo systemctl is-active containerd >/dev/null 2>&1; do echo "Waiting for containerd..."; sleep 10; done' || handle_error "Containerd service did not become active in time."
echo "Containerd is active."

# Perform the join operation.
echo "Running 'kubeadm join' to add this node to the control plane..."
echo "Command: sudo kubeadm join $JOIN_ARGS --control-plane --certificate-key [REDACTED] --v=5"
if sudo kubeadm join $JOIN_ARGS --control-plane --certificate-key "$CERTIFICATE_KEY" --v=5; then
  echo "Successfully joined the control plane!"
else
  handle_error "'kubeadm join' command failed. Check the logs above for details."
fi

# Final verification step.
echo "Verifying join by checking for /etc/kubernetes/admin.conf..."
if sudo test -f /etc/kubernetes/admin.conf; then
  echo "admin.conf found - join appears successful."
else
  # This is a non-fatal warning as sometimes config isn't created on join nodes.
  echo "WARNING: /etc/kubernetes/admin.conf not found after join, but kubelet.conf should exist."
fi

%{ endif ~}
# --- END OF CONDITIONAL LOGIC ---

echo "================================================="
echo "=== K8s Controller Configuration Successful ==="
echo "    Run finished at: $(date)"
echo "================================================="
